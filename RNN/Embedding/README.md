# Embedding and Word2Vec

## Concepts

* [Word Embeddings](https://www.youtube.com/watch?v=ZsLhh1mly9k)

* [Embedding Weight Matrix](https://www.youtube.com/watch?time_continue=4&v=KVCcG5v8fi0)

* [Word2Vec Notebook](https://www.youtube.com/watch?v=4cWzv3YiF_w)
	* Continuous Bag of Word (CBOW)
	* Skip-gram
	* [Word2Vec Mikolov](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bc56d28_word2vec-mikolov/word2vec-mikolov.pdf)
	* [Distributed Representation](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bc56da8_distributed-representations-mikolov2/distributed-representations-mikolov2.pdf)
* Pre-Notebook: Word2Vec, SkipGram
> Clone the repo from Github and open the notebook Skip_Grams_Exercise.ipynb in the word2vec-embeddings folder.
 * Notebook: Word2Vec, SkipGram
 * [Data & Subsampling](https://www.youtube.com/watch?time_continue=4&v=7SJXv2BQzZA)
 	* [Quiz1](images/quiz1.png)
 * [Subsampling Solution1](https://www.youtube.com/watch?v=YXruURuFD7g)
 * [Context Word Targets](https://www.youtube.com/watch?v=DJN9MzD7ctY)
 * [Batching Data, Solution](https://www.youtube.com/watch?time_continue=12&v=nu2rjLzt1HI)
 * [Word2Vec Model](https://www.youtube.com/watch?v=7BEYWhym8lI)
 * [Model & Validations](https://www.youtube.com/watch?time_continue=1&v=GKDCq8J76tM)
 * [Negative Sampling](https://www.youtube.com/watch?time_continue=15&v=gnCwdegYNsQ)
 * Pre-Notebook: Negative Sampling
 > Clone the repo from Github and open the notebook Negative_Sampling_Exercise.ipynb in the word2vec-embeddings folder.
 * Notebook: Negative Sampling
 * [SkipGramNeg, Model Definition](https://www.youtube.com/watch?time_continue=20&v=e7ZrzpyXNDs)
 * [Complete Model & Custom Loss](https://www.youtube.com/watch?time_continue=19&v=7SqNN_eUAdc)

